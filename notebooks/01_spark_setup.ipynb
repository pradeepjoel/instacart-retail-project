{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d6b51cf-7ff8-4aed-86c9-7c6efce9fdbb",
   "metadata": {},
   "source": [
    "<B>INSTALLING SPARK<B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4cb0f0-29a4-49ac-be45-55cb60ca18e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install pyspark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aa1350-b4cc-4052-83f3-dda2a5ce59a0",
   "metadata": {},
   "source": [
    "<B>This cell initializes the SparkSession, which is the entry point for using Apache Spark. The SparkSession manages the connection between Python and Spark’s execution engine. All Spark DataFrame operations in this project depend on this session. If Spark fails to start here, no further Spark-based processing can run.<B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c20f371f-1aa4-4f34-8e02-817f5c939c5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.182.65:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.1.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>InstacartRetailProject</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x150253380>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"InstacartRetailProject\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285043be-52c5-4e64-a848-4cc0a445eb68",
   "metadata": {},
   "source": [
    "###  Load Raw Instacart Data into Spark\n",
    "\n",
    "This cell loads the raw Instacart datasets from CSV files into Spark DataFrames.\n",
    "\n",
    "Each dataset represents a different part of the retail domain:\n",
    "- Order and product purchase data  \n",
    "- Product metadata  \n",
    "- Aisle information  \n",
    "- Department information  \n",
    "\n",
    "The data is loaded with headers and automatic schema inference for easier exploration.\n",
    "No transformations are performed here — this step is only for data ingestion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec24155f-d6bb-429f-a62d-8d6abbf6ccd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "order_products = spark.read.csv(\n",
    "    \"../data_raw/order_products__prior.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "products = spark.read.csv(\n",
    "    \"../data_raw/products.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "aisles = spark.read.csv(\n",
    "    \"../data_raw/aisles.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "departments = spark.read.csv(\n",
    "    \"../data_raw/departments.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654c2978-cdd2-469a-8dd5-ba357035b4a0",
   "metadata": {},
   "source": [
    "### Validate Order-Product Data\n",
    "\n",
    "This cell checks the structure and size of the `order_products` dataset.\n",
    "\n",
    "- `printSchema()` is used to verify column names and data types.\n",
    "- `count()` is used to confirm the number of records loaded.\n",
    "\n",
    "This validation step ensures the data was ingested correctly before moving to cleaning and merging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0c4ca0d-1a8c-4df2-b8e6-71daf22e0453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- add_to_cart_order: integer (nullable = true)\n",
      " |-- reordered: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32434489"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_products.printSchema()\n",
    "order_products.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48118fe7-f93f-4020-a483-18148ce9e19c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
