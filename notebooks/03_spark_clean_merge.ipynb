{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dae7ba76-e5ee-4630-9e04-f93020a08bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"InstacartRetailProject\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c43194e-a800-49a8-a271-31ed5ce8ec57",
   "metadata": {},
   "source": [
    "### Load Raw Instacart Data Without Schema Inference\n",
    "\n",
    "This cell loads the raw Instacart CSV files into Spark DataFrames without inferring data types.\n",
    "\n",
    "All columns are initially read as strings.\n",
    "This approach avoids incorrect type casting during ingestion and allows data types to be handled explicitly in later cleaning steps.\n",
    "\n",
    "At this stage, the focus is on safely ingesting raw data, not transforming it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e617e963-d3ad-42e6-a057-83fa95b7fedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_products = spark.read.csv(\n",
    "    \"../data_raw/order_products__prior.csv\",\n",
    "    header=True,\n",
    "    inferSchema=False\n",
    ")\n",
    "\n",
    "products = spark.read.csv(\n",
    "    \"../data_raw/products.csv\",\n",
    "    header=True,\n",
    "    inferSchema=False\n",
    ")\n",
    "\n",
    "aisles = spark.read.csv(\n",
    "    \"../data_raw/aisles.csv\",\n",
    "    header=True,\n",
    "    inferSchema=False\n",
    ")\n",
    "\n",
    "departments = spark.read.csv(\n",
    "    \"../data_raw/departments.csv\",\n",
    "    header=True,\n",
    "    inferSchema=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff427c4e-649a-47c7-8950-78bd55a7fcdb",
   "metadata": {},
   "source": [
    "### Enrich Product Data with Aisle and Department Information\n",
    "\n",
    "This cell combines product data with aisle and department metadata.\n",
    "\n",
    "- Products are joined with aisles using `aisle_id`\n",
    "- The result is further joined with departments using `department_id`\n",
    "- Left joins are used to ensure no products are lost during enrichment\n",
    "\n",
    "This creates a unified product dataset with full categorical context, which is required for downstream analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "165127f7-e32f-4b1f-8de5-7ab76bd84d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_full = products \\\n",
    "    .join(aisles, \"aisle_id\", \"left\") \\\n",
    "    .join(departments, \"department_id\", \"left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e93093-484c-41a2-82c8-dfe2c16805cf",
   "metadata": {},
   "source": [
    "### Create Transaction-Level Dataset\n",
    "\n",
    "This cell joins order-level purchase data with enriched product information.\n",
    "\n",
    "- Each row represents a product purchased within an order\n",
    "- Product details (aisle and department) are added to each transaction\n",
    "- A left join ensures all purchase records are preserved\n",
    "\n",
    "The resulting dataset forms the core transaction table used for enrichment, analysis, and modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd26ceba-9e7c-455f-983b-b59e4e5253eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = order_products.join(\n",
    "    products_full,\n",
    "    \"product_id\",\n",
    "    \"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc43006-1c95-4a3f-ac29-67f6ce70a0b5",
   "metadata": {},
   "source": [
    "### Validate Transaction Dataset\n",
    "\n",
    "This cell previews the merged transaction data and checks the total number of records.\n",
    "\n",
    "- `show(5)` displays sample rows to verify joins and column values\n",
    "- `count()` confirms the dataset size after merging\n",
    "\n",
    "This validation step ensures the transaction table was created correctly before further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0c25b32-103d-466b-8e30-10110b11e8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----------------+---------+-------------+--------+------------------+------------------+----------+\n",
      "|product_id|order_id|add_to_cart_order|reordered|department_id|aisle_id|      product_name|             aisle|department|\n",
      "+----------+--------+-----------------+---------+-------------+--------+------------------+------------------+----------+\n",
      "|     33120|       2|                1|        1|           16|      86|Organic Egg Whites|              eggs|dairy eggs|\n",
      "|     30035|       2|                5|        0|           13|      17| Natural Sweetener|baking ingredients|    pantry|\n",
      "|     17794|       2|                6|        1|            4|      83|           Carrots|  fresh vegetables|   produce|\n",
      "|     45918|       2|                4|        1|           13|      19|    Coconut Butter|     oils vinegars|    pantry|\n",
      "|      9327|       2|                3|        0|           13|     104|     Garlic Powder| spices seasonings|    pantry|\n",
      "+----------+--------+-----------------+---------+-------------+--------+------------------+------------------+----------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "32434489"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions.show(5)\n",
    "transactions.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8511b5b8-d725-4701-b00d-a07368bb461b",
   "metadata": {},
   "source": [
    "### Persist Cleaned Transactions Data\n",
    "\n",
    "This cell writes the cleaned and merged transaction dataset to disk in Parquet format.\n",
    "\n",
    "Parquet is a columnar storage format that is efficient for large-scale analytics and Spark processing.\n",
    "The overwrite mode ensures the output can be regenerated cleanly during development.\n",
    "\n",
    "This dataset becomes the official cleaned input for downstream enrichment and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38a85a26-c379-4948-89b3-997a39e9ddae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "transactions.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"../data_clean/transactions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947fe33d-16ea-4fd6-b5c3-7955c0fcbb43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
