{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e652420",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "PrOJECT_ROOT = Path.cwd()\n",
    "RAW_DATA_DIR = PrOJECT_ROOT / \"data\" / \"raw\"\n",
    "PROCESSED_DATA_DIR = PrOJECT_ROOT / \"data\" / \"processed\"\n",
    "\n",
    "PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "products = pd.read_csv(RAW_DATA_DIR / \"products.csv\")\n",
    "prior = pd.read_csv(RAW_DATA_DIR / \"order_products__prior.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "788acadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.DataFrame'>\n",
      "RangeIndex: 49688 entries, 0 to 49687\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype\n",
      "---  ------         --------------  -----\n",
      " 0   product_id     49688 non-null  int64\n",
      " 1   product_name   49688 non-null  str  \n",
      " 2   aisle_id       49688 non-null  int64\n",
      " 3   department_id  49688 non-null  int64\n",
      "dtypes: int64(3), str(1)\n",
      "memory usage: 3.0 MB\n"
     ]
    }
   ],
   "source": [
    "products.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bfd8c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.DataFrame'>\n",
      "RangeIndex: 32434489 entries, 0 to 32434488\n",
      "Data columns (total 4 columns):\n",
      " #   Column             Dtype\n",
      "---  ------             -----\n",
      " 0   order_id           int64\n",
      " 1   product_id         int64\n",
      " 2   add_to_cart_order  int64\n",
      " 3   reordered          int64\n",
      "dtypes: int64(4)\n",
      "memory usage: 989.8 MB\n"
     ]
    }
   ],
   "source": [
    "prior.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "004309b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>add_to_cart_order</th>\n",
       "      <th>reordered</th>\n",
       "      <th>product_name</th>\n",
       "      <th>aisle_id</th>\n",
       "      <th>department_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32387109</th>\n",
       "      <td>3416166</td>\n",
       "      <td>46516</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Sesame Bagels</td>\n",
       "      <td>93</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25727059</th>\n",
       "      <td>2713284</td>\n",
       "      <td>33768</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Sinfully Sweet Campari Tomatoes</td>\n",
       "      <td>83</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14262865</th>\n",
       "      <td>1505164</td>\n",
       "      <td>34864</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pet Waste Bags</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31773614</th>\n",
       "      <td>3351195</td>\n",
       "      <td>20015</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>Grandma's Hummus</td>\n",
       "      <td>67</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22136169</th>\n",
       "      <td>2334804</td>\n",
       "      <td>48423</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>Organic Sweet Peas</td>\n",
       "      <td>116</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          order_id  product_id  add_to_cart_order  reordered  \\\n",
       "32387109   3416166       46516                  4          0   \n",
       "25727059   2713284       33768                  4          1   \n",
       "14262865   1505164       34864                  1          1   \n",
       "31773614   3351195       20015                 20          1   \n",
       "22136169   2334804       48423                  6          0   \n",
       "\n",
       "                             product_name  aisle_id  department_id  \n",
       "32387109                    Sesame Bagels        93              3  \n",
       "25727059  Sinfully Sweet Campari Tomatoes        83              4  \n",
       "14262865                   Pet Waste Bags        40              8  \n",
       "31773614                 Grandma's Hummus        67             20  \n",
       "22136169               Organic Sweet Peas       116              1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = prior.merge(products, on=\"product_id\", how=\"inner\")\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2743798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.DataFrame'>\n",
      "RangeIndex: 32434489 entries, 0 to 32434488\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Dtype\n",
      "---  ------             -----\n",
      " 0   order_id           int64\n",
      " 1   product_id         int64\n",
      " 2   add_to_cart_order  int64\n",
      " 3   reordered          int64\n",
      " 4   product_name       str  \n",
      " 5   aisle_id           int64\n",
      " 6   department_id      int64\n",
      "dtypes: int64(6), str(1)\n",
      "memory usage: 2.5 GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56da4285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>product_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16389318</th>\n",
       "      <td>1728955</td>\n",
       "      <td>42964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29006545</th>\n",
       "      <td>3059228</td>\n",
       "      <td>17862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10381825</th>\n",
       "      <td>1096061</td>\n",
       "      <td>29487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12781439</th>\n",
       "      <td>1349062</td>\n",
       "      <td>17858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3486100</th>\n",
       "      <td>368325</td>\n",
       "      <td>45073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          order_id  product_id\n",
       "16389318   1728955       42964\n",
       "29006545   3059228       17862\n",
       "10381825   1096061       29487\n",
       "12781439   1349062       17858\n",
       "3486100     368325       45073"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We only need the order_id and product_id columns for the FP-Growth algorithm, so we will drop the other columns.\n",
    "df_fp = df[[\"order_id\", \"product_id\"]]\n",
    "df_fp.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05687051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.DataFrame'>\n",
      "RangeIndex: 32434489 entries, 0 to 32434488\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Dtype\n",
      "---  ------      -----\n",
      " 0   order_id    int64\n",
      " 1   product_id  int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 494.9 MB\n"
     ]
    }
   ],
   "source": [
    "df_fp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb4034c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "order_id      0\n",
       "product_id    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "df_fp.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a29f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicated rows\n",
    "print(\"Number of duplicated rows:\", df_fp.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7148169a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique orders: 3214874\n",
      "Number of unique products: 49677\n"
     ]
    }
   ],
   "source": [
    "# Check the number of unique orders and products\n",
    "print(f\"Number of unique orders: {df_fp['order_id'].nunique()}\")\n",
    "print(f\"Number of unique products: {df_fp['product_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68be2893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned DataFrame to a parquet file for later use in the FP-Growth algorithm.\n",
    "df_fp.to_parquet(\n",
    "    PROCESSED_DATA_DIR / \"transactions.parquet\",\n",
    "    engine=\"pyarrow\",\n",
    "    compression=\"snappy\",\n",
    "    index=False\n",
    ")\n",
    "# pyarrow is a fast and efficient library for reading and writing parquet files, while snappy is a compression algorithm that provides a good balance between compression ratio and speed. Setting index=False ensures that the DataFrame index is not included in the output file, which can save space and improve performance when reading the file later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401e40e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data which is saved in parquet format\n",
    "df_fp = pd.read_parquet(PROCESSED_DATA_DIR / \"transactions.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c5752e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.DataFrame'>\n",
      "RangeIndex: 32434489 entries, 0 to 32434488\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Dtype\n",
      "---  ------      -----\n",
      " 0   order_id    int64\n",
      " 1   product_id  int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 494.9 MB\n"
     ]
    }
   ],
   "source": [
    "df_fp.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe2ae8e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29d3877b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baskets with 1 item (Removed): 156,748\n",
      "Baskets with 2+ items (Retained): 3,058,126\n"
     ]
    }
   ],
   "source": [
    "# Calculate basket sizes\n",
    "basket_sizes = df_fp.groupby('order_id').size()\n",
    "\n",
    "# Filter for orders with 2 or more products\n",
    "multi_item_ids = basket_sizes[basket_sizes >= 2].index\n",
    "df_mining = df_fp[df_fp['order_id'].isin(multi_item_ids)]\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Baskets with 1 item (Removed): {len(basket_sizes) - len(multi_item_ids):,}\")\n",
    "print(f\"Baskets with 2+ items (Retained): {len(multi_item_ids):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a62f13",
   "metadata": {},
   "source": [
    "## Data Quality Filtering: Handling \"Single-Item\" Baskets\n",
    "\n",
    "### **Objective**\n",
    "In **Association Rule Mining** (FP-Growth/Apriori), we seek to discover relationships between products (e.g., \"If a customer buys A, they are likely to buy B\"). For a relationship to exist, an order must contain at least **two items**. \n",
    "\n",
    "### **The Logic**\n",
    "Orders containing only one item—while useful for calculating general product popularity—provide zero information regarding **product affinity**. Including them in the mining phase:\n",
    "1.  **Dilutes Support Metrics:** It makes items look less \"associative\" than they actually are.\n",
    "2.  **Increases Computational Overhead:** The algorithm spends time processing millions of rows that cannot physically produce a rule.\n",
    "\n",
    "### **Implementation**\n",
    "We group the data by `order_id` to calculate the \"basket size\" for every transaction. We then create a mask to retain only the orders that meet our **minimum complexity requirement** ($N \\ge 2$).\n",
    "\n",
    "\n",
    "\n",
    "### **output**\n",
    "* **Baskets Removed:** 156,748 (Single-item \"noise\")\n",
    "* **Baskets Retained:** 3,058,126 (High-quality multi-item transactions)\n",
    "* **Data Integrity:** By removing these, we ensure our **Confidence** and **Lift** scores are calculated against a baseline of actual shopping \"trips\" where choices were made between multiple products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "298e34ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 3,058,126 baskets.\n",
      "Example of a single basket: [33120, 28985, 9327, 45918, 30035, 17794, 40141, 1819, 43668]\n"
     ]
    }
   ],
   "source": [
    "# Create baskets for FP-Growth\n",
    "\n",
    "# We group by order_id and turn the product_ids into a list\n",
    "# This creates the exact format the FP-Growth library expects\n",
    "baskets = df_mining.groupby('order_id')['product_id'].apply(list).tolist()\n",
    "\n",
    "print(f\"Created {len(baskets):,} baskets.\")\n",
    "print(f\"Example of a single basket: {baskets[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80aec4cc",
   "metadata": {},
   "source": [
    "## Data Structuring: Generating Transactional Baskets\n",
    "\n",
    "### **Objective**\n",
    "To convert our relational DataFrame into a **Transactional \"List of Lists\"** format. This is the final structural requirement before the data can be ingested by the **MLxtend** library and the **FP-Growth** algorithm.\n",
    "\n",
    "### **The Transformation Logic**\n",
    "We are shifting the data from a \"Tall\" format (where each row is one product) to a \"Wide\" format (where each row is one complete order).\n",
    "\n",
    "1.  **`groupby('order_id')`**: This identifies all rows belonging to a single shopping trip.\n",
    "2.  **`['product_id'].apply(list)`**: Within each group, we harvest all the unique product IDs and pack them into a standard Python list.\n",
    "3.  **`.tolist()`**: We convert the resulting Pandas Series into a master list containing millions of sub-lists (one per order).\n",
    "\n",
    "### **Why This Format?**\n",
    "The **FP-Tree** (Frequent Pattern Tree) structure works by scanning through transactions one by one. By providing a list of lists, we allow the algorithm to:\n",
    "* Identify overlapping items across different transactions efficiently.\n",
    "* Build the tree nodes without the overhead of Pandas index lookups.\n",
    "* Preserve the **Product IDs** as the primary values for mining.\n",
    "\n",
    "\n",
    "\n",
    "### **Execution Metrics**\n",
    "* **Total Baskets:** 3,058,126\n",
    "* **Structure:** `[[ID1, ID2, ...], [ID3, ID4, ...], ...]`\n",
    "* **Example Output:** A single basket now looks like a digital receipt: `[33120, 28985, 9327, 45918, ...]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc47fe8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of a sorted basket: [1819, 9327, 17794, 28985, 30035, 33120, 40141, 43668, 45918]\n"
     ]
    }
   ],
   "source": [
    "# Sort the items within each basket\n",
    "# This makes the FP-Tree construction much more efficient\n",
    "baskets = [sorted(basket) for basket in baskets]\n",
    "\n",
    "print(f\"Example of a sorted basket: {baskets[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfdb1cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product_id\n",
      "24852    15.385828\n",
      "13176    12.317216\n",
      "21137     8.618448\n",
      "21903     7.872632\n",
      "47209     6.961355\n",
      "47766     5.765067\n",
      "47626     4.979062\n",
      "16797     4.641176\n",
      "26209     4.588693\n",
      "27845     4.477154\n",
      "Name: count, dtype: float64\n",
      "\n",
      "Locked in the Top 2000 products for the final matrix.\n"
     ]
    }
   ],
   "source": [
    "# First, let's see what we're working with. \n",
    "# value_counts() gives us the frequency for every product in the dataset.\n",
    "item_counts = df_mining['product_id'].value_counts()\n",
    "\n",
    "# Let's see top 10 popular items\n",
    "# If Bananas are at 15%, that's a huge anchor for our rules.\n",
    "item_percents = (item_counts / len(baskets)) * 100\n",
    "print(item_percents.head(10))\n",
    "\n",
    "# Focus on the Top 2000.\n",
    "# Why 2000? It covers the high-velocity items and keeps the matrix \n",
    "# small enough that my pc can handle it.\n",
    "TOP_N = 2000\n",
    "top_2000_set = set(item_counts.head(TOP_N).index)\n",
    "\n",
    "print(f\"\\nLocked in the Top {TOP_N} products for the final matrix.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cddf03f",
   "metadata": {},
   "source": [
    "## Feature Selection: The \"Top-N\" Frequency Filter\n",
    "\n",
    "### **Objective**\n",
    "To perform **Dimensionality Reduction** by isolating the most influential products in the dataset. This ensures our model focuses on high-velocity items that provide the strongest statistical \"signal.\"\n",
    "\n",
    "### **The \"Long Tail\" Logic**\n",
    "In the Instacart dataset, product popularity follows a **Power Law distribution**. While there are ~50,000 unique products, a small percentage (the \"Head\") drives the majority of transactions, while thousands of products (the \"Tail\") appear in fewer than 100 orders.\n",
    "* **Noise Reduction:** Rare items often create \"spurious correlations\"—coincidences that look like patterns but aren't statistically reliable.\n",
    "* **Computational Optimization:** Reducing the feature space to 2,000 products allows us to build a **Sparse Matrix** that is small enough for local memory while still capturing the vast majority of consumer behavior.\n",
    "\n",
    "### **Implementation Strategy**\n",
    "1.  **Frequency Analysis**: We calculate the \"Support\" for every item by dividing its occurrence by the total number of baskets.\n",
    "2.  **Top-K Selection**: we lock in the **Top 2,000 Product IDs** as our target features.\n",
    "3.  **Set Optimization**: We convert these IDs into a Python `set`. This is a critical engineering step; checking if an item exists in a **Set** is significantly faster ($O(1)$) than checking a **List** ($O(N)$), which is vital when processing 32 million rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa2ab46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned up! We have 2,761,063 baskets left for the algorithm.\n"
     ]
    }
   ],
   "source": [
    "# Now I need to clean the actual baskets. \n",
    "# I'm looping through every order and tossing out anything that isn't a \"Top 2000\" item.\n",
    "# Using a set lookup here because doing this 3 million times on a list would take time\n",
    "# set lookups works like a hash table and are O(1) on average, while list lookups are O(n).\n",
    "baskets_filtered = [[item for item in b if item in top_2000_set] for b in baskets]\n",
    "\n",
    "# if I remove items, some baskets might now have only 1 item (or zero).\n",
    "# I'll drop those now since you can't have an \"association\" with just one product.\n",
    "baskets_final = [b for b in baskets_filtered if len(b) >= 2]\n",
    "\n",
    "print(f\"Cleaned up! We have {len(baskets_final):,} baskets left for the algorithm.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62dfecf",
   "metadata": {},
   "source": [
    "## Data Pruning: Applying the Frequency Filter\n",
    "\n",
    "### **Objective**\n",
    "To synchronize our transactional baskets with our **Top-2000** feature selection. This step removes \"rare\" products from every order and performs a final quality check to ensure all remaining baskets are valid for mining.\n",
    "\n",
    "1.  **High-Speed Filtering ($O(1)$ Lookup)**: \n",
    "    * We iterate through over 3 million baskets. Inside each basket, we check if a product exists in our `top_2000_set`. \n",
    "    * By using a **Python Set** (which functions like a Hash Table), the time complexity for this check is **Constant Time ($O(1)$)**. If we used a standard list, the processing time would be exponentially higher ($O(N)$).\n",
    "2.  **Post-Pruning Validation**: \n",
    "    * Removing rare items can \"hollow out\" an order. For example, if a customer bought 1 popular banana and 2 rare spices, our filter removes the spices, leaving only the banana.\n",
    "    * Since **FP-Growth** requires at least two items to form a relationship ($A \\rightarrow B$), we perform a second pass to drop any baskets that were reduced to 0 or 1 items.\n",
    "\n",
    "\n",
    "\n",
    "### **Why This Matters**\n",
    "* **Dimensionality Alignment**: This ensures our final matrix has exactly 2,000 columns with no \"orphaned\" data.\n",
    "* **Statistical Reliability**: By removing the \"Long Tail\" items from individual baskets, we ensure that the **Support** and **Confidence** metrics of our final rules are not skewed by infrequent, one-off purchases.\n",
    "\n",
    "### **Current State**\n",
    "* **Input**: 3,058,126 baskets (unfiltered).\n",
    "* **Output**: 2,761,063 baskets (optimized).\n",
    "* **Retention**: Approximately **90.3%** of multi-item baskets were retained after filtering for the Top 2,000 products, indicating that our feature selection covers the vast majority of core consumer behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d3c3839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting baskets to a sparse matrix.\n",
      "Matrix is ready! Shape: (2761063, 2000)\n",
      "Approximate RAM usage: 100.64 MB\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "# I'm initializing the encoder to turn my list of lists into a True/False matrix.\n",
    "te = TransactionEncoder()\n",
    "\n",
    "print(\"Converting baskets to a sparse matrix.\")\n",
    "\n",
    "# The sparse=True argument is the most important part here. \n",
    "# It tells Python to only store the 'True' values, saving massive amounts of RAM.\n",
    "# how sparse=True works: Instead of storing a full matrix with mostly False values.\n",
    "te_ary = te.fit(baskets_final).transform(baskets_final, sparse=True)\n",
    "\n",
    "# I'll wrap this in a Sparse DataFrame so I can keep the product_id column names.\n",
    "df_encoded = pd.DataFrame.sparse.from_spmatrix(te_ary, columns=te.columns_)\n",
    "\n",
    "print(f\"Matrix is ready! Shape: {df_encoded.shape}\")\n",
    "print(f\"Approximate RAM usage: {df_encoded.memory_usage().sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9f86f5",
   "metadata": {},
   "source": [
    "## Logical Transformation: Sparse Boolean Encoding\n",
    "\n",
    "### **Objective**\n",
    "To transform our list of baskets into a **Binary Matrix** (One-Hot Encoding). This is the specific mathematical format required by the **FP-Growth** algorithm to calculate item relationships.\n",
    "\n",
    "### **The \"Sparse\" Solution**\n",
    "In a standard matrix, the computer would create a cell for every product and every order. With 2.7 million orders and 2,000 products, that is **5.4 billion cells**. \n",
    "* **The Problem**: A \"Dense\" matrix would store every single `False` value, requiring over **20GB of RAM**.\n",
    "* **The Solution**: We use `sparse=True`. This tells Python to only remember where the `True` values (the actual purchases) are located. \n",
    "\n",
    "\n",
    "\n",
    "### **Why This Step is Critical**\n",
    "1. **Memory Efficiency**: By ignoring the empty space (the products a customer *didn't* buy), we reduced our RAM usage from ~20GB to approximately **100MB**.\n",
    "2. **Preserving Identity**: We wrap the matrix in a **Sparse DataFrame**. This ensures that our column headers remain the original **Product IDs**, allowing us to identify the products later.\n",
    "3. **Speed**: The `mlxtend` TransactionEncoder is highly optimized for this specific transformation, preparing the data for mining in seconds rather than minutes.\n",
    "\n",
    "### **Technical Summary**\n",
    "* **Input**: A list of lists (`baskets_final`).\n",
    "* **Output**: A Compressed Sparse Row (CSR) matrix.\n",
    "* **Result**: A highly compressed, high-speed data structure ready for the **FP-Growth** algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "599274b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting FP-Growth mining with string-based columns\n",
      "Success Found 142 frequent itemsets.\n",
      "     support            itemsets\n",
      "14  0.168670  frozenset({24852})\n",
      "6   0.135097  frozenset({13176})\n",
      "15  0.094981  frozenset({21137})\n",
      "2   0.086748  frozenset({21903})\n",
      "7   0.076763  frozenset({47209})\n",
      "16  0.063589  frozenset({47766})\n",
      "39  0.054862  frozenset({47626})\n",
      "51  0.050876  frozenset({16797})\n",
      "66  0.050588  frozenset({26209})\n",
      "8   0.049288  frozenset({27966})\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "\n",
    "# The fpgrowth library has a bug with Sparse DataFrames when column names are integers.\n",
    "# I'm converting the Product IDs to strings to bypass this limitation.\n",
    "df_encoded.columns = [str(i) for i in df_encoded.columns]\n",
    "\n",
    "# Now we try the mining again.\n",
    "# I'm sticking with 1% (0.01) to see how many \"strong\" patterns we have.\n",
    "print(\"Starting FP-Growth mining with string-based columns\")\n",
    "frequent_itemsets = fpgrowth(df_encoded, min_support=0.01, use_colnames=True)\n",
    "\n",
    "# Sorting so the most popular itemsets are at the top.\n",
    "frequent_itemsets = frequent_itemsets.sort_values(by='support', ascending=False)\n",
    "\n",
    "print(f\"Success Found {len(frequent_itemsets)} frequent itemsets.\")\n",
    "print(frequent_itemsets.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4a9e86",
   "metadata": {},
   "source": [
    "## Pattern Discovery: Frequent Itemset Mining (FP-Growth)\n",
    "\n",
    "### **Objective**\n",
    "To identify **Frequent Itemsets**—groups of products that appear together in transactions more often than a specified threshold. We utilize the **FP-Growth (Frequent Pattern Growth)** algorithm for its superior speed and memory efficiency.\n",
    "\n",
    "### **The Algorithm: Why FP-Growth?**\n",
    "Unlike the older Apriori algorithm, FP-Growth does not generate \"candidate\" sets (potential combinations), which can be computationally expensive. Instead:\n",
    "1. It builds a compressed **FP-Tree** structure in memory.\n",
    "2. It fragments the tree into smaller \"conditional\" trees to extract frequent patterns.\n",
    "This approach is significantly faster for large datasets like Instacart's 3-million-basket collection.\n",
    "\n",
    "\n",
    "\n",
    "### **The Support Threshold**\n",
    "We have set the `min_support` to **0.01 (1%)**. \n",
    "* **Meaning**: A product or combination of products must appear in at least **1% of all transactions** (approx. 27,610 baskets) to be included in our results.\n",
    "* **Goal**: This high threshold ensures that we are looking at \"core\" consumer behaviors rather than rare, one-off occurrences.\n",
    "\n",
    "### **Implementation Note: The Integer Bug Fix**\n",
    "A technical workaround was required for this step. The `mlxtend` library has a known limitation when handling **Sparse DataFrames** with integer column names. \n",
    "* **The Fix**: We converted the **Product IDs** (e.g., `24852`) into **Strings** (e.g., `\"24852\"`). \n",
    "* **The Result**: This prevents the algorithm from confusing column labels with column indices, allowing the mining process to complete successfully.\n",
    "\n",
    "### **Analysis of Results**\n",
    "* **Success**: The algorithm identified **142 frequent itemsets**.\n",
    "* **Top Itemsets**: As expected, the results are dominated by \"anchor\" products like Bananas and Organic Produce, providing a statistically sound foundation for the next phase: generating Association Rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "587cbbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 38 association rules!\n",
      "           antecedents         consequents   support  confidence      lift\n",
      "27  frozenset({27966})  frozenset({21137})  0.012264    0.248835  2.619848\n",
      "26  frozenset({21137})  frozenset({27966})  0.012264    0.129126  2.619848\n",
      "25  frozenset({24852})  frozenset({28204})  0.012293    0.072885  2.263507\n",
      "24  frozenset({28204})  frozenset({24852})  0.012293    0.381785  2.263507\n",
      "19  frozenset({13176})  frozenset({27966})  0.014669    0.108584  2.203071\n",
      "18  frozenset({27966})  frozenset({13176})  0.014669    0.297628  2.203071\n",
      "0   frozenset({47209})  frozenset({13176})  0.022579    0.294133  2.177204\n",
      "1   frozenset({13176})  frozenset({47209})  0.022579    0.167129  2.177204\n",
      "36  frozenset({45066})  frozenset({24852})  0.010289    0.358072  2.122917\n",
      "37  frozenset({24852})  frozenset({45066})  0.010289    0.061000  2.122917\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "# We generate the rules based on our frequent itemsets.\n",
    "# I'm using 'lift' as the metric because it shows how much MORE likely \n",
    "# someone is to buy Y given X, compared to just buying Y randomly.\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n",
    "\n",
    "# Let's sort them by 'lift' to find the strongest, most \"surprising\" relationships.\n",
    "rules = rules.sort_values(by='lift', ascending=False)\n",
    "\n",
    "print(f\"Generated {len(rules)} association rules!\")\n",
    "# Showing the top 10 rules with the most important columns\n",
    "print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60699de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE FINAL ASSOCIATION RULES\n",
      "         antecedents_names       consequents_names   support  confidence  \\\n",
      "27     Organic Raspberries    Organic Strawberries  0.012264    0.248835   \n",
      "26    Organic Strawberries     Organic Raspberries  0.012264    0.129126   \n",
      "25                  Banana      Organic Fuji Apple  0.012293    0.072885   \n",
      "24      Organic Fuji Apple                  Banana  0.012293    0.381785   \n",
      "19  Bag of Organic Bananas     Organic Raspberries  0.014669    0.108584   \n",
      "18     Organic Raspberries  Bag of Organic Bananas  0.014669    0.297628   \n",
      "0     Organic Hass Avocado  Bag of Organic Bananas  0.022579    0.294133   \n",
      "1   Bag of Organic Bananas    Organic Hass Avocado  0.022579    0.167129   \n",
      "36        Honeycrisp Apple                  Banana  0.010289    0.358072   \n",
      "37                  Banana        Honeycrisp Apple  0.010289    0.061000   \n",
      "\n",
      "        lift  \n",
      "27  2.619848  \n",
      "26  2.619848  \n",
      "25  2.263507  \n",
      "24  2.263507  \n",
      "19  2.203071  \n",
      "18  2.203071  \n",
      "0   2.177204  \n",
      "1   2.177204  \n",
      "36  2.122917  \n",
      "37  2.122917  \n"
     ]
    }
   ],
   "source": [
    "# Load the product names\n",
    "products = pd.read_csv(RAW_DATA_DIR /'products.csv')\n",
    "\n",
    "# Create a dictionary for quick lookup\n",
    "id_to_name = dict(zip(products['product_id'].astype(str), products['product_name']))\n",
    "\n",
    "def get_names(ids_frozenset):\n",
    "    # Converts the frozenset of ID strings back to a readable string of names\n",
    "    return \", \".join([id_to_name.get(str(i), \"Unknown\") for i in ids_frozenset])\n",
    "\n",
    "# Apply the naming function to our rules\n",
    "rules['antecedents_names'] = rules['antecedents'].apply(get_names)\n",
    "rules['consequents_names'] = rules['consequents'].apply(get_names)\n",
    "\n",
    "# Display the Final Result\n",
    "final_report = rules[['antecedents_names', 'consequents_names', 'support', 'confidence', 'lift']]\n",
    "print(\"THE FINAL ASSOCIATION RULES\")\n",
    "print(final_report.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baed43fc",
   "metadata": {},
   "source": [
    "## Final Results:\n",
    "\n",
    "### **Objective**\n",
    "To decode the mathematical output of the **FP-Growth** algorithm into a human-readable format. By mapping **Product IDs** back to their original names, we can identify actionable consumer trends and cross-selling opportunities.\n",
    "\n",
    "### **The Decoding Logic**\n",
    "We utilized the `products.csv` metadata to create a high-speed lookup dictionary. Since our association rules were generated using string-based IDs (to avoid the integer bug), we implemented a mapping function that:\n",
    "1.  Accesses the `frozenset` of IDs for both **Antecedents** (the \"If\" item) and **Consequents** (the \"Then\" item).\n",
    "2.  Retrieves the corresponding **Product Name** from our dictionary.\n",
    "3.  Reformats the data into a clean, presentation-ready report.\n",
    "\n",
    "### **Key Metrics Explained**\n",
    "* **Support**: The percentage of all orders that contain both items. (e.g., ~1.2% for Organic Berries).\n",
    "* **Confidence**: The probability that the consequent is purchased given that the antecedent is already in the cart. \n",
    "    * *Insight*: **38%** of customers who buy **Organic Fuji Apples** also buy a **Banana**.\n",
    "* **Lift**: The strength of the association. \n",
    "    * *Insight*: A lift of **2.62** for **Organic Raspberries** and **Strawberries** means they are bought together **2.6 times more often** than if customers were picking items at random.\n",
    "\n",
    "[Image of association rule mining metrics showing support confidence and lift relationships]\n",
    "\n",
    "### **Final Analysis & Business Recommendations**\n",
    "The results reveal a powerful **\"Organic Cluster.\"** Customers purchasing organic produce are highly likely to stick within the organic ecosystem for their entire basket. \n",
    "\n",
    "**Strategic Recommendations:**\n",
    "1.  **Digital Merchandising**: Place \"Organic Raspberries\" as a recommended item on the product page for \"Organic Strawberries\" to capitalize on the 2.6x Lift.\n",
    "2.  **Bundle Promotions**: Create an \"Organic Fruit Bowl\" discount bundle featuring the top-performing pairs (Berries + Bananas).\n",
    "3.  **Store Layout**: In physical or \"Dark Store\" fulfillment centers, keep **Organic Hass Avocados** and **Bag of Organic Bananas** in close proximity to reduce picking time and encourage multi-item purchases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
